{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.utils import tokenize\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model=LdaModel.load('../gensim_data/trained_model.tmp')\n",
    "dictionary=Dictionary.load('../gensim_data/Dictionary.tmp')\n",
    "X=pd.read_csv('..\\..\\..\\..\\Local Data\\project_5_data\\\\aylien\\\\aylien_body_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow=SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a probability prediction from the LDA model of form\n",
    "#[(topic1, prob1), (topic2, prob2),...]  \n",
    "#returns the topic number with the the highest assigned probabiltiy.\n",
    "\n",
    "def probs_to_topic(probs):\n",
    "    assigned_topic=-1\n",
    "    max_prob=0\n",
    "    for topic, prob in probs:\n",
    "        if prob > max_prob:\n",
    "            assigned_topic=topic\n",
    "            max_prob=prob\n",
    "    return assigned_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Index Reference**\n",
    "(These are not exact rules, and are topics classified by the unsupervised trained LDA model)\n",
    "\n",
    "0: Global Warming/Drought/Climate disasters.\n",
    "\n",
    "1: Fires\n",
    "\n",
    "2: Earthquakes/Volcanos/Seismic Events\n",
    "\n",
    "3: Urban/Other (This is a weird one -- I think here were lots of airline accidents in the training data, and any article that talks about the urban ramifications of a disaster tends to get sorted here.).\n",
    "\n",
    "4: Storms/Hurricanes\n",
    "\n",
    "5: Floods/Rains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"water\" + 0.013*\"year\" + 0.009*\"climat\" + 0.007*\"chang\" + 0.006*\"it\" + 0.006*\"drought\" + 0.006*\"govern\" + 0.005*\"flood\" + 0.005*\"citi\" + 0.005*\"level\"'),\n",
       " (1,\n",
       "  '0.027*\"fire\" + 0.011*\"burn\" + 0.010*\"firefight\" + 0.009*\"australia\" + 0.009*\"bushfir\" + 0.008*\"south\" + 0.008*\"home\" + 0.008*\"state\" + 0.008*\"condit\" + 0.007*\"temperatur\"'),\n",
       " (2,\n",
       "  '0.029*\"earthquak\" + 0.017*\"magnitud\" + 0.015*\"quak\" + 0.013*\"report\" + 0.012*\"a\" + 0.011*\"mile\" + 0.011*\"damag\" + 0.010*\"erupt\" + 0.009*\"island\" + 0.008*\"hit\"'),\n",
       " (3,\n",
       "  '0.013*\"i\" + 0.007*\"it\" + 0.007*\"island\" + 0.006*\"we\" + 0.006*\"home\" + 0.006*\"t\" + 0.005*\"famili\" + 0.005*\"polic\" + 0.005*\"a\" + 0.005*\"hous\"'),\n",
       " (4,\n",
       "  '0.024*\"storm\" + 0.022*\"flood\" + 0.017*\"hurrican\" + 0.016*\"rain\" + 0.012*\"weather\" + 0.012*\"wind\" + 0.011*\"warn\" + 0.010*\"dorian\" + 0.007*\"south\" + 0.007*\"expect\"'),\n",
       " (5,\n",
       "  '0.021*\"flood\" + 0.014*\"district\" + 0.013*\"rain\" + 0.011*\"state\" + 0.011*\"water\" + 0.009*\"heavi\" + 0.008*\"india\" + 0.007*\"offici\" + 0.007*\"affect\" + 0.006*\"river\"')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing/Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input : for lack of a better assumption, let's assume that the input will be a dataframe that has one article per row,\n",
    "#        and a feature named \"body\" of it's unprocessed body text as a string.\n",
    "#        this could include title text as well, but didn't want to put too many assumptions on the input\n",
    "#        till i look into heroku\n",
    "\n",
    "#output: a dataframe with three columns: body text, token list, corpus (where the corpus is the token ids), \n",
    "##and predicted category\n",
    "\n",
    "def process_body(dataframe):\n",
    "    text_body=dataframe['body'].values\n",
    "    text_body=[remove_stopwords(body) for body in text_body]\n",
    "    text_body=[tokenize(body, deacc=\"True\", lowercase=\"True\") for body in text_body]\n",
    "    text_body=[[snow.stem(token) for token in word_list] for word_list in text_body]\n",
    "    body_df=dataframe[['body']]\n",
    "    body_df['tokens']=[list(gen) for gen in text_body]\n",
    "    body_df['corpus']=[dictionary.doc2bow(doc) for doc in body_df['tokens']]\n",
    "    body_df['predicted_topic']= [probs_to_topic(topic_probs)for topic_probs in trained_model.get_document_topics(disaster_body['corpus'])]\n",
    "    return body_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
